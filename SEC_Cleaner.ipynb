{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8592ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unstructured.cleaners.core import *\n",
    "from bs4 import NavigableString\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2104ffa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# headers needed to access SEC APIs\n",
    "headers = '{\"User-Agent\": test, email: test@test.net\"}' \n",
    "\n",
    "def get_soup(url, headers=headers):\n",
    "    # Use requests to fetch the content of the webpage\n",
    "    response = requests.get(url, headers=headers)\n",
    "    text = re.sub(r'&nbsp;', ' ', response.text)\n",
    "    # Use BeautifulSoup to parse the HTML content\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def extract_pages(soup):\n",
    "    \n",
    "    tables = []\n",
    "    images = []\n",
    "    \n",
    "    soup = preprocess_soup(soup, tables, images)\n",
    "\n",
    "    page_breaks = soup.find_all('hr')\n",
    "\n",
    "    raw_pages = []\n",
    "    # Iterate through the range, stop before the last break\n",
    "    # skipping the last page that is for signatures.\n",
    "    for i in range(len(page_breaks)-1):\n",
    "        page = soup.new_tag('page')\n",
    "        # Get all tags between page_breaks[i] and page_breaks[i + 1]\n",
    "        current_break = page_breaks[i]\n",
    "        next_break = page_breaks[i + 1]\n",
    "        # Extract elements and move them to the 'page' tag\n",
    "        element = current_break.find_next_sibling()\n",
    "        while element and element != next_break:\n",
    "            next_element = element.find_next_sibling()\n",
    "            page.append(element.extract())\n",
    "            element = next_element\n",
    "       \n",
    "        raw_pages.append(page)\n",
    "    pages_soup = []\n",
    "    \n",
    "    for page in raw_pages:  \n",
    "        html_text = [str(p) for p in page]\n",
    "        pages_soup.append(BeautifulSoup(''.join(html_text), 'html.parser'))       \n",
    "    return pages_soup\n",
    "\n",
    "def preprocess_soup(soup, tables, images):\n",
    "    #add marker to bold and italic text\n",
    "    soup = add_markers_around_bold(soup)\n",
    "    soup = add_markers_around_italic(soup)\n",
    "    # preprocess\n",
    "    soup = preprocess_inline_new_lines(soup)\n",
    "    soup = preprocess_tables(soup, tables)\n",
    "    soup = preprocess_images(soup, images)\n",
    "    soup = preprocess_paragraph_new_lines(soup)\n",
    "    return soup\n",
    "\n",
    "def is_bold(tag):\n",
    "        # Checks if a tag is explicitly bold or has a bold style\n",
    "    if tag.name in [\"b\", \"strong\"]:\n",
    "        return True    \n",
    "    if tag.has_attr('style') and is_style_bold(tag['style']):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_style_bold(style_str):\n",
    "    styles = [style.strip().lower().replace(' ','') for style in style_str.split(';')]\n",
    "    return any('font-weight:bold' in style or 'font-weight:700' in style for style in styles)\n",
    "\n",
    "def add_markers_around_bold(soup):\n",
    "    for tag in soup.find_all(is_bold):\n",
    "        tag.insert(0, NavigableString('[BOLD]'))\n",
    "        tag.append(NavigableString('[BOLD_END]'))\n",
    "    return soup\n",
    "\n",
    "def is_italic(tag):\n",
    "    # Checks if a tag is explicitly italic or has an italic style\n",
    "    if tag.name in [\"i\", \"em\"]:\n",
    "        return True   \n",
    "    if tag.has_attr('style') and is_style_italic(tag['style']):\n",
    "        return True    \n",
    "    return False\n",
    "                   \n",
    "def is_style_italic(style_str):\n",
    "    styles = [style.strip().lower().replace(' ','') for style in style_str.split(';')]\n",
    "    return any('font-style:italic' in style for style in styles)\n",
    "\n",
    "\n",
    "def add_markers_around_italic(soup):\n",
    "    for tag in soup.find_all(is_italic):\n",
    "        tag.insert(0, NavigableString('[ITALIC]'))\n",
    "        tag.append(NavigableString('[ITALIC_END]'))\n",
    "    return soup\n",
    "\n",
    "def preprocess_tables(soup, tables): \n",
    "    def count_non_empty_row(tag):\n",
    "        non_empty_rows_count = 0\n",
    "        for row in tag.find_all('tr'):\n",
    "            if row.get_text(strip=True):\n",
    "                non_empty_rows_count += 1\n",
    "        return non_empty_rows_count\n",
    "\n",
    "    for tag in tqdm(soup.find_all('table'), desc=\"Preprocess table elements\"):  \n",
    "        new_p_tag = soup.new_tag(\"p\")\n",
    "        if count_non_empty_row(tag) == 1:\n",
    "            new_p_tag.string = f'{tag.get_text()}'.replace(\"\\n\",\"\")\n",
    "        else:\n",
    "            new_p_tag.string = f'\\n\\n[TABLE_REPLACED_{len(tables)}]\\n\\n'\n",
    "            tables.append(tag)     \n",
    "        tag.insert_after(new_p_tag)\n",
    "        tag.decompose()\n",
    "    return soup\n",
    "\n",
    "def preprocess_images(soup, images):\n",
    "    for tag in soup.find_all('img'):  \n",
    "        new_p_tag = soup.new_tag(\"p\")\n",
    "        new_p_tag.string = f'[IMAGE_REPLACED_{len(images)}]'\n",
    "        images.append(tag)\n",
    "        tag.insert_after(new_p_tag)\n",
    "        tag.decompose\n",
    "    return soup\n",
    "               \n",
    "def preprocess_inline_new_lines(soup):\n",
    "    \"\"\"\n",
    "    Recursively process a BS4 tag to replace '\\n' with ' ' in inline elements\n",
    "    without preformatted text and with no relevant CSS white-space property.\n",
    "    \"\"\"\n",
    "    inline_elements = ['span', 'p', 'font', 'a', 'b', 'i', 'em', 'strong', 'u', 'small', 'sub', 'sup']  \n",
    "    inline_tags = soup.find_all(inline_elements)\n",
    "    \n",
    "    for tag in soup.find_all(inline_elements):  \n",
    "        style = tag.attrs.get('style', '')\n",
    "        # Check if the style does not contain 'white-space: pre' before processing\n",
    "        if 'white-space: pre' not in style:\n",
    "            for content in tag.contents:\n",
    "                if isinstance(content, NavigableString):\n",
    "                    # Replace newline characters in the string\n",
    "                    new_content = content.replace('\\n', ' ')\n",
    "                    content.replace_with(new_content)\n",
    "    return soup\n",
    "\n",
    "def preprocess_paragraph_new_lines(soup):\n",
    "    \"\"\"\n",
    "    Recursively process a BS4 tag to append '\\n\\n' at the end of all specified block-level elements.\n",
    "    \"\"\"\n",
    "    newline_tags = ['div', 'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'hr', \n",
    "                    'blockquote', 'pre', 'dl', 'dt', 'dd', #'ul', 'ol', 'li',\n",
    "                    'table', 'form', 'address', 'figure', 'figcaption']\n",
    "    for tag in soup.find_all(newline_tags):  \n",
    "        # Check if the tag is not empty or doesn't already end with '\\n\\n'\n",
    "        if tag.contents and not str(tag.contents[-1]).endswith('\\n\\n'):\n",
    "            # Append '\\n\\n' as a new NavigableString\n",
    "            tag.append(NavigableString('\\u00A0 \\n\\n'))\n",
    "    return soup\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8826ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm  \n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"sp500_ten_years_documents.json\"\n",
    "\n",
    "# Load the JSON file back into a list of dictionaries\n",
    "with open(file_path, 'r') as json_file:\n",
    "    document_dictionaries = json.load(json_file)\n",
    "\n",
    "\n",
    "sample = random.choice(document_dictionaries)\n",
    "url=sample['form_link']\n",
    "\n",
    "print(sample['company_name'], sample['year'])\n",
    "print(url)\n",
    "\n",
    "\n",
    "USER_AGENT = 'test@test1.com'\n",
    "headers = {'User-Agent': USER_AGENT}\n",
    "soup = get_soup(url)\n",
    "\n",
    "print('Preparing Soup')\n",
    "pages_soup = extract_pages(soup)\n",
    "print('Pages Soup Ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1cdea8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/906345/000090634519000006/cpt1231201810k.htm\n",
      "CPU times: user 63.2 ms, sys: 6.96 ms, total: 70.2 ms\n",
      "Wall time: 75.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import unstructured\n",
    "from unstructured.cleaners.core import *\n",
    "from IPython.display import HTML\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "marker_dict = {\n",
    "        '[BOLD]': '[BOLD_END]',\n",
    "        '[ITALIC]': '[ITALIC_END]',\n",
    "        # Add more markers as needed\n",
    "    }\n",
    "\n",
    "marker_list = [marker for markers in marker_dict.items() for marker in markers]\n",
    "\n",
    "\n",
    "def select_one_marker(text, marker, marker_dict=marker_dict):\n",
    "    if marker not in text:\n",
    "        return re.sub(r'\\s+', ' ', text.strip()) \n",
    "    d_copy = marker_dict.copy()\n",
    "    del d_copy[marker]\n",
    "    markers_to_remove = list(d_copy.keys()) + list(d_copy.values())\n",
    "    t = text\n",
    "    for m in markers_to_remove:\n",
    "        text = text.replace(m, '')\n",
    "    return re.sub(r'\\s+', ' ', text.strip()) \n",
    "\n",
    "\n",
    "def clean_markers(text, marker_dict=marker_dict):\n",
    "    # First, remove empty markers or markers with only whitespace between them\n",
    "    for open_marker, close_marker in marker_dict.items():\n",
    "        esc_open_marker = re.escape(open_marker)\n",
    "        esc_close_marker = re.escape(close_marker)\n",
    "        # Pattern to match markers with only whitespace or nothing between them\n",
    "        empty_pattern = f\"{esc_open_marker}\\\\s*{esc_close_marker}\"\n",
    "        text = re.sub(empty_pattern, \"\", text)\n",
    "    \n",
    "    # Then, handle cases of adjacent markers or nested markers without relevant content\n",
    "    # This might need a more nuanced approach depending on the exact behavior you want\n",
    "    for open_marker, close_marker in marker_dict.items():\n",
    "        esc_open_marker = re.escape(open_marker)\n",
    "        esc_close_marker = re.escape(close_marker)\n",
    "        # Adjust pattern to consider nested markers or directly adjacent markers\n",
    "        nested_pattern = f\"({esc_close_marker})\\\\s*({esc_open_marker})\"\n",
    "        # Simple approach: Replace directly adjacent markers with a single space or other logic as needed\n",
    "        text = re.sub(nested_pattern, \" \", text)  # Adjust this as per your requirement\n",
    "   \n",
    "    openers = list(marker_dict.keys())\n",
    "    closers = list(marker_dict.values())\n",
    "    text = remove_consecutive_markers(text, openers, closers, marker_dict)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_consecutive_markers(text, openers, closers, marker_dict):\n",
    "    for close_marker in closers:\n",
    "        close_opener = close_marker[:-5]+']'  \n",
    "        for open_marker in openers:\n",
    "            if close_marker == open_marker:  # Skip if it's the same marker\n",
    "                continue\n",
    "            esc_close_marker = re.escape(close_marker)\n",
    "            esc_open_marker = re.escape(open_marker)\n",
    "            esc_close_opener = re.escape(close_opener)\n",
    "            # Pattern to match the specific sequence: closer + optional whitespace + opener + optional whitespace + close_opener\n",
    "            # This time, we'll capture the opener to ensure it's not removed\n",
    "            pattern = f\"({esc_close_marker})\\\\s*({esc_open_marker})\\\\s*{esc_close_opener}\"\n",
    "            # Use a replacement function to keep the opener while removing the closer and close_opener\n",
    "            def replacement(match):\n",
    "                return match.group(2)  # Keep only the opener\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_removable_entry(text, marker_dict=marker_dict):\n",
    "    keywords = {\n",
    "        \"TOC\",\"TABLE OF CONTENT\", \"TABLE OF CONTENTS\", \"INDEX\",\n",
    "        \"INDEX TO FINANCIAL STATEMENTS\",\n",
    "        \"PART I\",\"PART II\",\"PART III\",\"PART I.\",\"PART II.\",\"PART III.\",\n",
    "        \"PART 1\",\"PART 2\",\"PART 3\",\"PART 1.\",\"PART 2.\",\"PART 3.\"}\n",
    "    for k, v in marker_dict.items():\n",
    "        text = text.replace(k, '')\n",
    "        text = text.replace(v, '')\n",
    "        \n",
    "\n",
    "    text_upper = text.strip().upper()\n",
    "    return text_upper in keywords\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def remove_numbers_and_substrings(text):\n",
    "    # Remove numbers up to three digits\n",
    "    text = re.sub(r'\\b\\d{1,3}\\b', '', text)\n",
    "    \n",
    "    # Define a list of substrings to remove\n",
    "    substrings_to_remove = ['page', 'number', 'of', 'nr', 'nr.', 'n', 'n.']\n",
    "    \n",
    "    # Construct a regex pattern to match any of the substrings\n",
    "    pattern = r'\\b(?:' + '|'.join(re.escape(sub) for sub in substrings_to_remove) + r')\\b'\n",
    "    \n",
    "    # Remove the matched substrings\n",
    "    text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text.strip()  # Added strip to remove leading/trailing whitespace\n",
    "\n",
    "def remove_numbers_and_words(text):\n",
    "    # Define the pattern to match 1 to 3 digit numbers or the specific words\n",
    "    # \\b ensures that we are matching whole words only (word boundary)\n",
    "    pattern = r'\\b(\\d{1,3}|page|number|of|nr|n)\\b'\n",
    "    \n",
    "    # Use re.sub to replace the found patterns with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove extra spaces that may be left after removals\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def remove_headers(pages, n=3, threshold=5):\n",
    "    pages_dicts = []\n",
    "    string_count = {}\n",
    "\n",
    "    # Step 1: Creating dictionaries for the first n elements of each page with masked values\n",
    "    for page in pages:\n",
    "        page_dict = {}\n",
    "        for i, original_string in enumerate(page[:n]):\n",
    "            masked_string = mask_string(original_string)  # Mask the string for counting\n",
    "            page_dict[i] = original_string  # Store original string in the dict\n",
    "            string_count[masked_string] = string_count.get(masked_string, 0) + 1\n",
    "        pages_dicts.append(page_dict)\n",
    "\n",
    "    # Step 2: Identifying masked strings with a count higher than the threshold\n",
    "    strings_above_threshold = {string for string, count in string_count.items() if count > threshold}\n",
    "\n",
    "    # Step 3: Removing headers based on masked strings\n",
    "    cleaned_pages = []\n",
    "    for page_dict, page in zip(pages_dicts, pages):\n",
    "        cleaned_page = []\n",
    "        for index, original_string in page_dict.items():\n",
    "            masked_string = mask_string(original_string)  # Mask the string to check against threshold\n",
    "            if masked_string not in strings_above_threshold:\n",
    "                cleaned_page.append(original_string)\n",
    "        # Append the rest of the page if it's longer than n\n",
    "        if len(page) > n:\n",
    "            cleaned_page.extend(page[n:])\n",
    "        cleaned_pages.append(cleaned_page)\n",
    "\n",
    "    return cleaned_pages\n",
    "\n",
    "\n",
    "def mask_string(string):\n",
    "    # Remove all numbers up to 3 digits, specific words, and all whitespaces\n",
    "    s = string.lower()\n",
    "    s = re.sub(r'\\b(page|number|nr|or)\\b', '', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\d{1,3}', '', s)\n",
    "    s = re.sub(f\"[ |i|v|x|l|c]\", '', s)\n",
    "    return s if (s[:4] not in ['item', '[tab', '[img']) else string\n",
    "\n",
    "def remove_footers(pages, n=3, threshold=5):\n",
    "    pages_dicts = []\n",
    "    string_count = {}\n",
    "\n",
    "    # Step 1: Creating dictionaries for the last n elements of each page with masked values\n",
    "    for page in pages:\n",
    "        page_dict = {}\n",
    "        for i, original_string in enumerate(page[-n:]):\n",
    "            masked_string = mask_string(original_string)  # Mask the string for counting\n",
    "            page_dict[-(i + 1)] = original_string  # Store original string in the dict\n",
    "            string_count[masked_string] = string_count.get(masked_string, 0) + 1\n",
    "        pages_dicts.append(page_dict)\n",
    "\n",
    "    # Step 2: Identifying masked strings with a count higher than the threshold\n",
    "    strings_above_threshold = {string for string, count in string_count.items() if count > threshold}\n",
    "\n",
    "    # Step 3: Removing footers based on masked strings\n",
    "    cleaned_pages = []\n",
    "    for page_dict, page in zip(pages_dicts, pages):\n",
    "        cleaned_page = list(page)  # Start with a full copy of the original page\n",
    "        for index, original_string in page_dict.items():\n",
    "            masked_string = mask_string(original_string)  # Mask the string to check against threshold\n",
    "            if masked_string in strings_above_threshold:\n",
    "                # Remove the original string from the cleaned page\n",
    "                cleaned_page.remove(original_string)\n",
    "        cleaned_pages.append(cleaned_page)\n",
    "\n",
    "    return cleaned_pages\n",
    "\n",
    "def clean_list(text):\n",
    "    # This pattern matches list markers at the start of the string or after a newline.\n",
    "    # It captures digits followed by a period, or various bullet characters, at the start of a line.\n",
    "    pattern = r'(^|\\n)(\\d+\\.\\s*|[\\*\\-\\+\\•\\•\\•\\■\\□\\○\\●\\◇\\◆\\▷\\◁]\\s*)'\n",
    "    \n",
    "    # Function to replace the matched markers with a standardized format.\n",
    "    def replacer(match):\n",
    "        start, marker = match.groups()\n",
    "        if marker.strip().isdigit():\n",
    "            # Keep ordered lists as they are, ensuring there's exactly one space.\n",
    "            new_marker = f'{start}{marker.rstrip()} '\n",
    "        else:\n",
    "            # Replace unordered list markers with a bullet and ensure one space.\n",
    "            new_marker = f'{start}- '\n",
    "        return new_marker\n",
    "    \n",
    "    # Replace the markers in the text based on the pattern and replacer function.\n",
    "    cleaned_text = re.sub(pattern, replacer, text, flags=re.MULTILINE)\n",
    "    \n",
    "    return cleaned_text\n",
    "    \n",
    "    return text\n",
    "def preprocess_page_text_sections(text):\n",
    "    # standardadize start and end new lines\n",
    "    text = text.strip()\n",
    "    text += '\\n\\n'  \n",
    "\n",
    "    def clean_text_before_listing(t):\n",
    "        t = re.sub(r'\\s*\\n\\s*\\n\\s*', '\\n\\n', t)\n",
    "        return t\n",
    "    text_list = [clean_text_before_listing(t) for t in text.split('\\n\\n') if t.strip() and not is_removable_entry(t)]\n",
    "\n",
    "    \n",
    "    clean_text = []\n",
    "    for t in text_list:\n",
    "        t = t.strip()\n",
    "        t = re.sub(r'\\s{2,}', ' ', t)\n",
    "        t = replace_unicode_quotes(t)\n",
    "        t = clean_markers(t)\n",
    "        t = clean_extra_whitespace(t)\n",
    "        t = re.sub(r'\\s*([,.;:?!\\'\"\\)])', r'\\1', t)\n",
    "        t = clean_list(t)\n",
    "        t_mask = t\n",
    "\n",
    "        clean_text.append(t)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def preproces_pages(page_soup):\n",
    "    pages = []\n",
    "    for page in pages_soup:\n",
    "#         print(page.prettify())\n",
    "        text = page.get_text(strip=False)\n",
    "        text = re.sub(r'\\xa0', ' ', text)\n",
    "        text = re.sub(r'(\\s?\\n){1}(\\s?\\n){2,}', '\\n\\n', text)\n",
    "        text_list = preprocess_page_text_sections(text)\n",
    "        pages.append(text_list)\n",
    "        \n",
    "\n",
    "    pages = remove_headers(pages)\n",
    "    pages = remove_footers(pages)\n",
    "    \n",
    "\n",
    "    return pages\n",
    "\n",
    "    \n",
    "pages = preproces_pages(pages_soup)\n",
    "\n",
    "sections = [section for page in pages for section in page]\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8dbbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_markers(text, marker_dict=marker_dict):\n",
    "    clean_text = text  # Start with the original text\n",
    "    for k, v in marker_dict.items():\n",
    "        clean_text = clean_text.replace(k, '')  # Remove the opening marker\n",
    "        clean_text = clean_text.replace(v, '')  # Remove the closing marker\n",
    "    return re.sub(r'\\s+', ' ', clean_text.strip()) \n",
    "\n",
    "def aggregate_lists(sections, marker_dict=marker_dict):\n",
    "    processed_sections = []\n",
    "    i = 0\n",
    "    while i < len(sections):\n",
    "        section = sections[i]\n",
    "        # Remove markers from the section for checking purposes\n",
    "        cleaned_section = ignore_markers(section, marker_dict)\n",
    "        # Use the original section for appending\n",
    "        aggregated_section = section\n",
    "\n",
    "        # Check if the cleaned section starts with '- '\n",
    "        if cleaned_section.startswith('- ') or cleaned_section.startswith(' - '):\n",
    "            # Join consecutive sections that start with '- '\n",
    "            while i + 1 < len(sections):\n",
    "                next_cleaned_section = ignore_markers(sections[i + 1], marker_dict)\n",
    "                if next_cleaned_section.startswith('- ') or cleaned_section.startswith(' - '):\n",
    "                    aggregated_section += '\\n' + sections[i + 1]  # Use the original next section\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        processed_sections.append(aggregated_section)\n",
    "        i += 1\n",
    "    return processed_sections\n",
    "    \n",
    "    \n",
    "    \n",
    "sections = aggregate_lists(sections, marker_dict=marker_dict)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91318bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emanueleferrari/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/906345/000090634519000006/cpt1231201810k.htm\n",
      "CPU times: user 906 ms, sys: 263 ms, total: 1.17 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer, AutoModel, AutoTokenizer\n",
    "import logging\n",
    "\n",
    "print(url)\n",
    "\n",
    "item_dict = {\n",
    "    'Item 1': ['ITEM 1 '],\n",
    "    'Item 1 and 2': ['ITEM 1 AND 2 ', 'ITEM 1 & 2 ', 'ITEMS 1 AND 2 ', 'ITEMS 1 & 2 '],\n",
    "     'Item 1A': ['ITEM 1A ', 'ITEM 1 A '],\n",
    "     'Item 1B': ['ITEM 1B ', 'ITEM 1 B '],\n",
    "     'Item 2': ['ITEM 2 '],\n",
    "     'Item 3': ['ITEM 3 '],\n",
    "     'Item 4': ['ITEM 4 '],\n",
    "     'Item 5': ['ITEM 5 '],\n",
    "     'Item 6': ['ITEM 6 '],\n",
    "     'Item 7': ['ITEM 7 '],\n",
    "     'Item 7A': ['ITEM 7A ', 'ITEM 7 A '],\n",
    "     'Item 8': ['ITEM 8 '],\n",
    "     'Item 9': ['ITEM 9 '],\n",
    "     'Item 9A': ['ITEM 9A ', 'ITEM 9 A '],\n",
    "     'Item 9B': ['ITEM 9B ', 'ITEM 9 B '],\n",
    "     'Item 9C': ['ITEM 9C ', 'ITEM 9 C '],\n",
    "     'Item 10': ['ITEM 10 '],\n",
    "     'Item 11': ['ITEM 11 '],\n",
    "     'Item 12': ['ITEM 12 '],\n",
    "     'Item 13': ['ITEM 13 '],\n",
    "     'Item 14': ['ITEM 14 '],\n",
    "     'Item 15': ['ITEM 15 '],\n",
    "     'Item 16': ['ITEM 16 ']}\n",
    "\n",
    "item_notations = [item for sublist in item_dict.values() for item in sublist]\n",
    "\n",
    "def item_standardization(text):\n",
    "    # Replace specified characters with whitespace\n",
    "    cleaned_text = re.sub(r'[.,\\-/_\\(\\)\\[\\]{}\\\\]', ' ', text)\n",
    "    # Replace multiple consecutive whitespaces with a single whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    cleaned_text =  cleaned_text.upper()\n",
    "    return cleaned_text\n",
    "\n",
    "def assign_title_levels(df):\n",
    "    # Condition 1\n",
    "    mask_condition_1 = (df['bold_all'] == True) & (df['italic_all'] == False) & (df['text'].str.isupper())\n",
    "    df.loc[mask_condition_1, 'title_level'] = 9\n",
    "    # Condition 2\n",
    "    mask_condition_2 = (df['bold_all'] == True) & (df['italic_all'] == False) & (~df['text'].str.isupper())\n",
    "    df.loc[mask_condition_2, 'title_level'] = 8\n",
    "    # Condition 3\n",
    "    mask_condition_3 = (df['bold_all'] == True) & (df['italic_all'] == True)\n",
    "    df.loc[mask_condition_3, 'title_level'] = 7\n",
    "    mask_condition_32 = (df['bold_all'] == False) & (df['italic_all'] == True)\n",
    "    df.loc[mask_condition_32, 'title_level'] = 6\n",
    "\n",
    "    # Condition 4\n",
    "    mask_condition_4 = (df['bold_all'] == False) & (df['italic_all'] == False) & (df['bold_start'] == True) & (df['italic_start'] == False)\n",
    "    df.loc[mask_condition_4, 'title_level'] = 5\n",
    "    # Condition 5\n",
    "    mask_condition_5 = (df['bold_all'] == False) & (df['italic_all'] == False) & (df['bold_start'] == True) & (df['italic_start'] == True)\n",
    "    df.loc[mask_condition_5, 'title_level'] = 4\n",
    "    # Condition 6\n",
    "    mask_condition_6 = (df['bold_all'] == False) & (df['italic_all'] == False) & (df['bold_start'] == False) & (df['italic_start'] == True)\n",
    "    df.loc[mask_condition_6, 'title_level'] = 3\n",
    "    # Set type based on title_level\n",
    "    df.loc[(df['title_level'] >= 7), 'type'] = 'title'\n",
    "    df.loc[(df['type'] == 'list'), 'title_level'] = 0\n",
    "    df.loc[~df['item'].isna(), 'title_level'] = 11\n",
    "    df['title_level'] = df['title_level'].fillna(2)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_item_key(text, item_dict):\n",
    "    \"\"\"\n",
    "    Function to find the key of the item notation that matches the beginning of the standardized text.\n",
    "    \"\"\"\n",
    "    standardized_text = item_standardization(text)  # Standardize the text\n",
    "    for key, notations in item_dict.items():  # Iterate over item_dict\n",
    "        for notation in notations:  # Check each notation for a match\n",
    "            if standardized_text.startswith(notation):  # If a match is found\n",
    "                return key  # Return the key (item name)\n",
    "    return np.nan  # Return NaN if no match is found\n",
    "\n",
    "def process_text_for_list_element(text, marker_dict=marker_dict):\n",
    "    # Combine all markers into a single pattern for easy removal\n",
    "    all_markers_pattern = '|'.join(re.escape(marker) for marker in list(marker_dict.keys()) + list(marker_dict.values()))\n",
    "\n",
    "    # Pattern to check the start condition and presence of a newline followed by zero or more markers\n",
    "    start_check_pattern = rf'^({all_markers_pattern})*\\s*-\\s+| - '\n",
    "    newline_check_pattern = rf'(\\n)({all_markers_pattern})*'\n",
    "\n",
    "    # Check if the text meets the start condition and contains the required newline pattern\n",
    "    if re.match(start_check_pattern, text) and re.search(newline_check_pattern, text):\n",
    "        # Remove all specified markers from the text\n",
    "        cleaned_text = re.sub(all_markers_pattern, '', text)\n",
    "\n",
    "        # Add '[LIST_ELEMENT]' at the beginning of the string\n",
    "        result_text = '[LIST_ELEMENT]' + cleaned_text\n",
    "    else:\n",
    "        # If conditions are not met, return the original text\n",
    "        result_text = text\n",
    "\n",
    "    return result_text\n",
    "\n",
    "\n",
    "def word_count(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    word_list = cleaned_text.strip().split()\n",
    "    return len(word_list)\n",
    "\n",
    "# Function to count tokens using BERT tokenizer\n",
    "def token_count(text, tokenizer):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def assign_id(df):\n",
    "    # Find the index of the row that matches the condition\n",
    "    condition_index = df[(df['item'].isin(['Item 1', 'Item 1 and 2'])) & (df['type'] == 'ItemMarker')].index[0]\n",
    "    \n",
    "    # Calculate IDs for rows before the condition_index\n",
    "    df.loc[:condition_index, 'id'] = range(-1, -condition_index - 2, -1)\n",
    "    \n",
    "    # Calculate IDs for rows from the condition_index onwards\n",
    "    df.loc[condition_index:, 'id'] = range(1, len(df) - condition_index + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def assign_parent_id(df):\n",
    "    # Initialize the 'parent_id' column with None (or any other default value)\n",
    "    df['parent_id'] = None\n",
    "    \n",
    "    # Reverse iterate over the DataFrame indices\n",
    "    for i in reversed(df.index):\n",
    "        current_title_level = df.loc[i, 'title_level']\n",
    "        \n",
    "        # Look for the first row with a higher 'title_level' above the current row\n",
    "        for j in reversed(df.index[:i]):\n",
    "            if df.loc[j, 'title_level'] > current_title_level:\n",
    "                # Assign the 'id' of the parent row instead of its index\n",
    "                df.loc[i, 'parent_id'] = df.loc[j, 'id']\n",
    "                break  # Stop searching once the parent is found\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_dataframe(sections, marker_list=marker_list):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "    # Prepare data for pandas\n",
    "    data = []\n",
    "    for string in sections:\n",
    "        cleaned_text = string\n",
    "\n",
    "        cleaned_text = process_text_for_list_element(cleaned_text)\n",
    "        if cleaned_text.startswith('[LIST_ELEMENT]'):\n",
    "            for m in marker_list:\n",
    "                cleaned_text = cleaned_text.replace(m,'')\n",
    "            data.append({'text': cleaned_text[14:], 'type': 'list', 'marked_text': string.replace('[LIST_ELEMENT]','')})\n",
    "            continue\n",
    "\n",
    "        cleaned_text = select_one_marker(cleaned_text, '[BOLD]')\n",
    "        cleaned_text = ignore_markers(string)\n",
    "        data.append({'text': cleaned_text, 'type': 'NarrativeText', 'marked_text': string})\n",
    "\n",
    "\n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create marks variables\n",
    "    df['bold_all'] = df['marked_text'].apply(lambda text: select_one_marker(text, '[BOLD]').startswith('[BOLD]') and\n",
    "                                                          select_one_marker(text, '[BOLD]').endswith('[BOLD_END]') and \n",
    "                                                          text.count('[BOLD]') == 1)\n",
    "    df['bold_start'] = df['marked_text'].apply(lambda text: select_one_marker(text, '[BOLD]').startswith('[BOLD]'))\n",
    "    df['bold_has_any'] = df['marked_text'].apply(lambda text: text.count('[BOLD]') > 0)\n",
    "    # TODO POSITION OF ALL MARKED TEXT\n",
    "    df['italic_all'] = df['marked_text'].apply(lambda text: select_one_marker(text, '[ITALIC]').startswith('[ITALIC]') and \n",
    "                                                            select_one_marker(text, '[ITALIC]').endswith('[ITALIC_END]') and \n",
    "                                                            text.count('[ITALIC]') == 1)\n",
    "    df['italic_start'] = df['marked_text'].apply(lambda text: select_one_marker(text, '[ITALIC]').startswith('[ITALIC]'))\n",
    "    df['italic_has_any'] = df['marked_text'].apply(lambda text: text.count('[ITALIC]') > 0)\n",
    "\n",
    "\n",
    "    # Identify Items\n",
    "    df['item'] = df.apply(lambda x: find_item_key(x['text'], item_dict) if x['bold_all'] else np.nan, axis=1)\n",
    "    if len(df[~df['item'].isna()])!= df['item'].nunique() or df['item'].nunique() <= 10:\n",
    "        raise ValueError(\"item labelling error \", (df['item'].value_counts()))\n",
    "\n",
    "    # Create title levels and items\n",
    "    df = assign_title_levels(df)\n",
    "    df.loc[~df['item'].isna(), 'type'] = 'ItemMarker'\n",
    "    df['item'] = df['item'].fillna(method='ffill')\n",
    "    df['item'] = df['item'].fillna('PreText')\n",
    "    df.loc[df['text'].str.startswith('[TABLE_'), 'type'] = 'table'\n",
    "    df.loc[df['text'].str.startswith('[IMAGE_'), 'type'] = 'image'\n",
    "\n",
    "    # Create Token Counter\n",
    "    # suppress log for token size over 512\n",
    "    original_logging_level = logging.getLogger(\"transformers\").getEffectiveLevel()\n",
    "    logging.getLogger(\"transformers\").setLevel(logging.ERROR)  # Suppress warnings\n",
    "    # apply token_count\n",
    "    df['token_count'] = df['text'].apply(lambda t: token_count(t, tokenizer))\n",
    "    # Restore the original logging level\n",
    "    logging.getLogger(\"transformers\").setLevel(original_logging_level) \n",
    "\n",
    "    # Other Counters\n",
    "    df['word_count'] = df['text'].apply(word_count)\n",
    "    df['char_count'] = df['text'].apply(len)\n",
    "    \n",
    "    # Add headers of text before first item\n",
    "    new_row_values = {'type':'ItemMarker','text':'Content before First Item.',\n",
    "                      'marked_text':'[BOLD][ITALIC] Content before First Item . [BOLD_END][ITALIC_END]',\n",
    "                      'item':'PreText','title_level': 11,\n",
    "                      'token_count':5,'word_count':4,'char_count':28,\n",
    "                      'bold_all':True,'bold_start':True,'bold_has_any':True,\n",
    "                      'italic_all':True,'italic_start':True,'italic_has_any':True}\n",
    "\n",
    "    new_row_df = pd.DataFrame([new_row_values], columns=df.columns)\n",
    "    df = pd.concat([new_row_df, df]).reset_index(drop=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df = assign_id(df)\n",
    "    df = assign_parent_id(df)\n",
    "\n",
    "\n",
    "    # reorder columns\n",
    "    df = df[['id', 'parent_id', 'type', 'text', 'marked_text','item', 'title_level',\n",
    "         'token_count', 'word_count', 'char_count',\n",
    "         'bold_all', 'bold_start', 'bold_has_any', 'italic_all', 'italic_start', 'italic_has_any',\n",
    "        ]]\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_dataframe(sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d84798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_markers(text):\n",
    "    # Replace bold markers\n",
    "    text = re.sub(r'\\[BOLD\\]', '<b>', text)\n",
    "    text = re.sub(r'\\[BOLD_END\\]', '</b>', text)\n",
    "    # Replace italic markers\n",
    "    text = re.sub(r'\\[ITALIC\\]', '<i>', text)\n",
    "    text = re.sub(r'\\[ITALIC_END\\]', '</i>', text)\n",
    "    # Replace newline characters with <br> tags\n",
    "    text = re.sub(r'\\n', '<br>', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def rebuild_html(s):\n",
    "    html_sections = []\n",
    "    s = replace_markers(s)\n",
    "    tag = '<div>' + s + '</div>'\n",
    "    html_sections.append(tag)\n",
    "#     display((HTML(s)))\n",
    "\n",
    "    return html_sections\n",
    "    \n",
    "        \n",
    "sections_html = [rebuild_html(s) for s in aggregate_lists(sections)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485550b3",
   "metadata": {},
   "source": [
    "## Future cleaning notes\n",
    "- table preprocessing\n",
    "    - one column table to multiple strings\n",
    "    - two column \n",
    "        - if first one is list char (one or 2 non whitespaces or number, abcd..) \n",
    "        - if first one is text and have one element each row first column and if not\n",
    "    - check if they have headers\n",
    "\n",
    "- store linkks image tables\n",
    "\n",
    "Part, Items (mind item 1 and 2 can be merged)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd29cd0",
   "metadata": {},
   "source": [
    "### Document with ussues examples\n",
    "\n",
    "\n",
    "https://www.sec.gov/Archives/edgar/data/8670/000000867022000038/adp-20220630.htm\n",
    "\n",
    "double column text at 'A major natural disaster or catastrophic event could have a materially adverse effect on our business, financial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf393bf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
